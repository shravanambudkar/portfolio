{"cells":[{"metadata":{"_cell_guid":"622a4e6a-6320-42e7-b440-308622757a50","_uuid":"57574d3cf6b5436f94b729e34df28f9037df481d"},"cell_type":"markdown","source":"**Interpret Sign Language with Deep Learning**\n* Which sign represents which letter in the alphabet?  \n* Looks like a task for [Deep Learning](https://www.kaggle.com/learn/deep-learning)!"},{"metadata":{"_cell_guid":"23c15724-cef9-4bc3-b46d-390740315e58","_uuid":"f685685f00e5ee14dd498de075c4725b57853399"},"cell_type":"markdown","source":"American Sign Language (ASL) is a natural language that serves as the predominant sign language of Deaf communities in the United States and most of Anglophone Canada.  ASL possesses a set of 26 signs known as the American manual alphabet, which can be used to spell out words from the English language.  Source: https://en.wikipedia.org/wiki/American_Sign_Language\n\nIn this kernel I demonstrate that deep learning can be used to interpret these signs. "},{"metadata":{"_cell_guid":"8ac5aba9-de61-4c1a-9197-0806bcd223b5","_uuid":"7f9547358c9cebf0a42166738c7dff19b16ff916"},"cell_type":"markdown","source":"*Step 1: Import Modules*"},{"metadata":{"_cell_guid":"42b35245-93b6-45ed-bcf8-d9ff22473269","_kg_hide-input":true,"_uuid":"3d3bc91774b6b395666c22dc2cca97af6d5dcbe3","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import models, layers, optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.utils import class_weight\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.applications.inception_v3 import InceptionV3\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport zlib\nimport itertools\nimport sklearn\nimport itertools\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport csv\nfrom tqdm import tqdm\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n#from keras.applications.mobilenet import MobileNet\n#from sklearn.metrics import roc_auc_score\n#from sklearn.metrics import roc_curve\n#from sklearn.metrics import auc\n#import warnings\n#warnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"59dcc7b7-740e-4ecf-a8ac-c86e66ea3511","_uuid":"be534235b529040019854353c2f3a373300cfb20"},"cell_type":"markdown","source":"*Step 2: Load Data*"},{"metadata":{"_kg_hide-input":true,"_uuid":"3d09099669d742877a4fa55379ab1e57371d91c5","collapsed":true,"_cell_guid":"683ed81d-5f4a-4b61-a401-8380e3c6cc3e","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# print(os.listdir(\"../input\"))\n# print(os.listdir(\"../input/asl-alphabet\"))\n# print(os.listdir(\"../input/asl-alphabet/asl_alphabet_train\"))\n# print(os.listdir(\"../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"))\n# print(os.listdir(\"../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"86a1fb25-c9b2-41fe-8bc3-01d91f7054bb","_uuid":"22c127e3183a316ca314946688e21db95a7dc4ca","trusted":true},"cell_type":"code","source":"imageSize=50\ntrain_dir = \"../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/\"\ntest_dir =  \"../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test/\"\nfrom tqdm import tqdm\ndef get_data(folder):\n    \"\"\"\n    Load the data and labels from the given folder.\n    \"\"\"\n    X = []\n    y = []\n    for folderName in os.listdir(folder):\n        if not folderName.startswith('.'):\n            if folderName in ['A']:\n                label = 0\n            elif folderName in ['B']:\n                label = 1\n            elif folderName in ['C']:\n                label = 2\n            elif folderName in ['D']:\n                label = 3\n            elif folderName in ['E']:\n                label = 4\n            elif folderName in ['F']:\n                label = 5\n            elif folderName in ['G']:\n                label = 6\n            elif folderName in ['H']:\n                label = 7\n            elif folderName in ['I']:\n                label = 8\n            elif folderName in ['J']:\n                label = 9\n            elif folderName in ['K']:\n                label = 10\n            elif folderName in ['L']:\n                label = 11\n            elif folderName in ['M']:\n                label = 12\n            elif folderName in ['N']:\n                label = 13\n            elif folderName in ['O']:\n                label = 14\n            elif folderName in ['P']:\n                label = 15\n            elif folderName in ['Q']:\n                label = 16\n            elif folderName in ['R']:\n                label = 17\n            elif folderName in ['S']:\n                label = 18\n            elif folderName in ['T']:\n                label = 19\n            elif folderName in ['U']:\n                label = 20\n            elif folderName in ['V']:\n                label = 21\n            elif folderName in ['W']:\n                label = 22\n            elif folderName in ['X']:\n                label = 23\n            elif folderName in ['Y']:\n                label = 24\n            elif folderName in ['Z']:\n                label = 25\n            elif folderName in ['del']:\n                label = 26\n            elif folderName in ['nothing']:\n                label = 27\n            elif folderName in ['space']:\n                label = 28           \n            else:\n                label = 29\n            for image_filename in tqdm(os.listdir(folder + folderName)):\n                img_file = cv2.imread(folder + folderName + '/' + image_filename)\n                if img_file is not None:\n                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n                    img_arr = np.asarray(img_file)\n                    X.append(img_arr)\n                    y.append(label)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X,y\nX_train, y_train = get_data(train_dir) \n#X_test, y_test= get_data(test_dir) # Too few images\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2) \n\n# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny_trainHot = to_categorical(y_train, num_classes = 30)\ny_testHot = to_categorical(y_test, num_classes = 30)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"416ed5f0-5aa3-4d80-a91b-3f16b450a495","_uuid":"676667e7c2b33f134ed685202b2f79bffa3b1734","collapsed":true,"trusted":true},"cell_type":"code","source":"# Shuffle data to permit further subsampling\nfrom sklearn.utils import shuffle\nX_train, y_trainHot = shuffle(X_train, y_trainHot, random_state=13)\nX_test, y_testHot = shuffle(X_test, y_testHot, random_state=13)\nX_train = X_train[:30000]\nX_test = X_test[:30000]\ny_trainHot = y_trainHot[:30000]\ny_testHot = y_testHot[:30000]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9697e49e-842c-4036-ae9f-641046758573","_uuid":"6a6b491f3ab910d04a2e7053eb8fb50eac2713c3"},"cell_type":"markdown","source":"*Step 3: Vizualize Data*"},{"metadata":{"_cell_guid":"a8175a28-50e7-4ef0-bdce-7d45de647677","_uuid":"23b61840058209bb797359e6b9eed686b5ecf3ac"},"cell_type":"markdown","source":"The min/max pixel values are already scaled between 0 and 1"},{"metadata":{"_cell_guid":"1abb596c-5c13-4d48-aaf2-d4683f822511","_kg_hide-input":false,"_uuid":"eb614459b47542a02b0bf9241d778922d6800a88","_kg_hide-output":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"def plotHistogram(a):\n    \"\"\"\n    Plot histogram of RGB Pixel Intensities\n    \"\"\"\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\nplotHistogram(X_train[1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee1a586a-7c7d-4e05-9341-5b7405849492","_uuid":"5328b63c34231563f8bfc2229f8b6efe76291c0e"},"cell_type":"markdown","source":"3 images from category \"A\""},{"metadata":{"_cell_guid":"ca9e3937-1a11-423a-9615-64e56220225a","_uuid":"1d8945520f8763653c798ccfa7a7e270367cf09f","trusted":true,"collapsed":true},"cell_type":"code","source":"multipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/**')\ndef plotThreeImages(images):\n    r = random.sample(images, 3)\n    plt.figure(figsize=(16,16))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(r[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(r[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(r[2]))\n    #;\nplotThreeImages(multipleImages)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4960ed7e-83cf-40c8-8432-751c4bd1171f","_uuid":"7c85c074c15fc66461e1f1c1e79109e0e44e94e8"},"cell_type":"markdown","source":"3 images from category \"B\""},{"metadata":{"_cell_guid":"4bbcc163-30f4-4cc5-b342-6ed23ba1e91c","_uuid":"ef1782c861f2729a13388699889be666d331f150","trusted":true,"collapsed":true},"cell_type":"code","source":"multipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/**')\ndef plotThreeImages(images):\n    r = random.sample(images, 3)\n    plt.figure(figsize=(16,16))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(r[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(r[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(r[2])) \nplotThreeImages(multipleImages)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"199e4466-a943-4de8-892e-c9ba068265c8","_uuid":"16f6d7087f9523d1633ecf65985c091ce8d64ba7"},"cell_type":"markdown","source":"20 images from category \"A\""},{"metadata":{"_cell_guid":"cd7271b9-db7e-4077-b7e0-82a7d182a223","_uuid":"2be42c9523cd6e2434a43ac5515736101971aa9e","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"A\")\nmultipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/**')\ni_ = 0\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nfor l in multipleImages[:25]:\n    im = cv2.imread(l)\n    im = cv2.resize(im, (128, 128)) \n    plt.subplot(5, 5, i_+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    i_ += 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38549df9-904c-4433-8ce1-a6ac9b0edb9d","_uuid":"306a5f90b75adc690d0d2eeda2a2eeca9397bc67"},"cell_type":"markdown","source":"20 images from category \"B\""},{"metadata":{"_cell_guid":"2fe9a400-5199-4092-84e1-dc4e96c0c2cb","_uuid":"fd135ae7a55ff56d6a7bc7ec01535b1a2d857f71","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"B\")\nmultipleImages = glob('../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/**')\ni_ = 0\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nfor l in multipleImages[:25]:\n    im = cv2.imread(l)\n    im = cv2.resize(im, (128, 128)) \n    plt.subplot(5, 5, i_+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    i_ += 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"acafe27f-6c13-4091-9ce0-907c5784eb79","_uuid":"6384bf60c740fc5cc97c90c6bb4a170e294210b2","trusted":true,"collapsed":true},"cell_type":"code","source":"map_characters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z', 26: 'del', 27: 'nothing', 28: 'space', 29: 'other'}\ndict_characters=map_characters\nimport seaborn as sns\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9414e78c-08b4-4c5c-b8df-73240e7c1350","_uuid":"83b4d12d206885ead7b84145c25027942a0f5b65"},"cell_type":"markdown","source":"*Step 4: Define Helper Functions*"},{"metadata":{"_cell_guid":"5c2b5fc4-e1af-4dfe-a928-8a8076c73d59","_uuid":"992129dbd3c7695bdd2e2497a6a56da0227c8c0d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Helper Functions  Learning Curves and Confusion Matrix\n\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nclass MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n\ndef plotKerasLearningCurve():\n    plt.figure(figsize=(10,5))\n    metrics = np.load('logs.npy')[()]\n    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n        l = np.array(metrics[k])\n        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n        y = l[x]\n        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n    plt.legend(loc=4)\n    plt.axis([0, None, None, None]);\n    plt.grid()\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Accuracy')\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (8,8))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef plot_learning_curve(history):\n    plt.figure(figsize=(8,8))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./accuracy_curve.png')\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('./loss_curve.png')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d9c6929-3f64-4d4e-a82b-362602641156","_uuid":"46be241c508bd8f733fd41b84fa0d4d12ff67b33"},"cell_type":"markdown","source":"*Step 5: Evaluate Classification Models*"},{"metadata":{"_cell_guid":"3dac612a-0543-47a7-b9be-9a45820b0473","_uuid":"7bb6dff5a30e1644bbfffa0a7c7b5992df5a494c"},"cell_type":"markdown","source":"Transfer learning w/ VGG16 Convolutional Network"},{"metadata":{"_cell_guid":"da473dc4-7e79-4be0-97aa-c7cca6e8aa43","_uuid":"1b8d8acad18ea6c063c61c50d84c5c65f8678b21","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"map_characters1 = map_characters\nclass_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nweight_path1 = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nweight_path2 = '../input/keras-pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model_1 = VGG16(weights = weight_path1, include_top=False, input_shape=(imageSize, imageSize, 3))\n#pretrained_model_2 = InceptionV3(weights = weight_path2, include_top=False, input_shape=(imageSize, imageSize, 3))\noptimizer1 = keras.optimizers.Adam()\noptimizer2 = keras.optimizers.RMSprop(lr=0.0001)\ndef pretrainedNetwork(xtrain,ytrain,xtest,ytest,pretrainedmodel,pretrainedweights,classweight,numclasses,numepochs,optimizer,labels):\n    base_model = pretrained_model_1 # Topless\n    # Add top layer\n    x = base_model.output\n    x = Flatten()(x)\n    predictions = Dense(numclasses, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    # Train top layer\n    for layer in base_model.layers:\n        layer.trainable = False\n    model.compile(loss='categorical_crossentropy', \n                  optimizer=optimizer, \n                  metrics=['accuracy'])\n    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n    model.summary()\n    # Fit model\n    history = model.fit(xtrain,ytrain, epochs=numepochs, class_weight=classweight, validation_data=(xtest,ytest), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n    # Evaluate model\n    score = model.evaluate(xtest,ytest, verbose=0)\n    print('\\nKeras CNN - accuracy:', score[1], '\\n')\n    y_pred = model.predict(xtest)\n    print('\\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(y_pred, axis=1), target_names=list(labels.values())), sep='') \n    Y_pred_classes = np.argmax(y_pred,axis = 1) \n    Y_true = np.argmax(ytest,axis = 1) \n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    plotKerasLearningCurve()\n    plt.show()\n    plot_learning_curve(history)\n    plt.show()\n    plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))\n    plt.show()\n    return model\npretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,10,optimizer1,map_characters1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4df933c-a0c0-435e-baeb-42fbec811394","_uuid":"f8647a019c1fe3ed13a4866fe9314b3cb833f278"},"cell_type":"markdown","source":"Great, we were able to interpret the signs with an accuracy rate of approximately 92%.  That is much better than random chance given that there were 26 different signs!"},{"metadata":{"_kg_hide-input":true,"_uuid":"d1d747b2c84f43929f9fb75af8f739248b90279d","collapsed":true,"_cell_guid":"1468fed2-8194-4d54-84d1-fbd7549ecdf0","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# To Do: (1) try using more than 30000 of the 87000 images; (2) try using larger images; (3) try using different network architectures ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}